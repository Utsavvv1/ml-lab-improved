# Transformer Implementation (Attention Is All You Need)

This repository contains a faithful PyTorch implementation of the Transformer model from the landmark paper ["Attention Is All You Need" (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762).

In addition to the standard architecture, this project implements modern optimizations (such as **Pre-Layer Normalization**) to improve training stability, with a roadmap for further state-of-the-art enhancements.

## ğŸš€ Current Status & Features

### 1\. Baseline Architecture (Implemented)

We have successfully replicated the core components of the original Transformer:

  * **Multi-Head Attention**: 8 parallel heads with scaled dot-product attention.
  * **Encoder-Decoder Stack**: Full 6-layer stack with residual connections.
  * **Positional Encodings**: Standard sinusoidal geometric progression for handling sequence order.
  * **Embeddings**: Shared weights between input/output embeddings and the final linear projection.
  * **Regularization**: Residual Dropout ($P_{drop}=0.1$) and Label Smoothing ($\epsilon_{ls}=0.1$).
  * **Optimizer**: Custom `NoamOpt` scheduler with linear warmup and inverse square root decay.

### 2\. Modern Improvements (Implemented)

  * **Pre-Layer Normalization (Pre-LN)**:
      * **Problem:** The original Post-LN placement often leads to unstable gradients at initialization.
      * **Solution:** We moved Layer Normalization to the *input* of the sublayers (inside the residual block).
      * **Benefit:** Training is significantly more stable and robust to hyperparameter changes.
      * *Configuration:* Toggled via `layer_norm_mode="pre_ln"` (default) or `post_ln`.

## ğŸ“‚ Project Structure

```text
ml-lab-improved/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model.py       # Main Transformer, Encoder, Decoder classes
â”‚   â”œâ”€â”€ layers.py      # EncoderLayer and DecoderLayer
â”‚   â”œâ”€â”€ blocks.py      # Attention, Pre-LN/Post-LN Sublayers, FeedForward
â”‚   â””â”€â”€ embeddings.py  # Standard Sinusoidal Positional Encoding
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ loader.py      # TextDataLoader for src-trg pairs
â”‚   â””â”€â”€ batches.py     # Masking and Batch object handling
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ optimizer.py   # Noam Learning Rate Scheduler
â”‚   â”œâ”€â”€ loss.py        # Label Smoothing Loss
â”‚   â””â”€â”€ tokenizer.py   # BPE Tokenizer wrapper
â”œâ”€â”€ train.py           # Training loop (Synthetic Copy Task & Real World)
â”œâ”€â”€ inference.py       # Beam Search & Greedy Decoding
â””â”€â”€ verify_changes.py  # Unit tests for Architecture & Pre-LN logic
```

## ğŸ› ï¸ Installation

1.  **Clone and Setup Environment:**
    You can use the provided PowerShell script to set up the virtual environment automatically:

    ```powershell
    ./setup.ps1
    ```

2.  **Manual Installation:**

    ```bash
    python -m venv .venv
    source .venv/bin/activate  # or .venv\Scripts\activate
    pip install -r requirements.txt
    ```

## ğŸƒ Usage

### 1\. Run Verification Tests

Ensure all components (Weight Sharing, Pre-LN logic, Beam Search) are functioning correctly:

```bash
python verify_changes.py
```

### 2\. Train on Synthetic Data (Copy Task)

To demonstrate the model's ability to learn simple patterns:

```bash
python run_train.py
```

*This uses the default configuration in `config.json`.*

### 3\. Train on Real Data

To train on your own source/target text files:

```bash
python train.py path/to/source.txt path/to/target.txt
```

### 4\. Inference

Run inference using Beam Search on a trained model:

```bash
python inference.py
```

## ğŸ—ºï¸ Roadmap & Planned Improvements

Based on our [Implementation Progress Report](https://www.google.com/search?q=ml_progress_report.pdf), the following features are planned for the next release:

  * **[Planned] Rotary Positional Embeddings (RoPE)**: Replacing absolute sinusoidal encodings with relative positioning to improve generalization on variable sequence lengths[cite: 39, 42].
  * **[Planned] GeLU Activation**: Replacing `ReLU` with Gaussian Error Linear Units to avoid the "dying ReLU" problem[cite: 46, 48].
  * **[Planned] FlashAttention / Linear Attention**: Optimizing the attention mechanism from $O(n^2)$ to $O(n)$ complexity for longer sequence processing[cite: 24, 26].

## ğŸ“œ License

MIT License. See `LICENSE` for details.
